{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34adb893-2d28-4ea2-b32d-7ca8951362c2",
   "metadata": {},
   "source": [
    "# NLP Tutorial for Begginers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e365547-8817-46b1-80ff-66e9f104cac0",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and humans using natural language. It aims to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
    "\n",
    "NLP has a wide range of applications across various industries, including healthcare, finance, customer service, and more. Here are some common applications of NLP:\n",
    "\n",
    "1. Sentiment Analysis: NLP can be used to analyze and interpret the sentiment of text data, such as social media posts, customer reviews, and news articles. This can help companies understand how their customers feel about their products or services and make informed decisions based on this feedback.\n",
    "\n",
    "2. Chatbots: NLP is used to develop chatbots that can interact with users in a natural language format. These chatbots can provide customer support, answer questions, and even make recommendations based on user input.\n",
    "\n",
    "3. Machine Translation: NLP is used in machine translation tools like Google Translate to translate text from one language to another. These tools use algorithms to understand the meaning of the text and produce accurate translations.\n",
    "\n",
    "4. Information Extraction: NLP can be used to extract relevant information from unstructured text data, such as emails, reports, and social media posts. This information can then be used for various purposes, such as data analysis, trend forecasting, and decision-making.\n",
    "\n",
    "5. Speech Recognition: NLP is used in speech recognition systems like Siri and Alexa to convert spoken language into text. These systems use algorithms to understand and interpret the spoken words, enabling users to interact with their devices using voice commands.\n",
    "\n",
    "Overall, NLP has revolutionized the way we interact with technology and has opened up new possibilities for businesses and individuals alike. As the field continues to advance, we can expect to see even more innovative applications of NLP in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e39aaae-3177-409f-a01d-46628172dd90",
   "metadata": {},
   "source": [
    "## NLP Data Preprocessing:\n",
    "Natural Language Processing (NLP) involves making sense of human language through computational techniques. One of the key aspects of building effective NLP models is data preprocessing. NLP data preprocessing involves cleaning and transforming raw text data into a format that is suitable for analysis, training machine learning models, and extracting valuable insights. In this notebook, we'll explore the importance of NLP data preprocessing and various techniques involved in preparing text data for NLP tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84773824-3e28-41ac-883b-6df522d11adc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Techniques for NLP Data Preprocessing\n",
    "\n",
    "### 1. Text Cleaning\n",
    "Removing HTML tags, URLs, special characters, and non-alphabetic characters can help in cleaning text data and making it more suitable for analsysi\n",
    "### 2. Text Normalization\n",
    "Converting text to lowercase, handling contractions, expanding abbreviations, and lemmatization (reducing words to their base forms) are common normalization techniques in NLP preprocessis\n",
    "\n",
    "### 3. Text Tokenization\n",
    "Splitting text into tokens (words or sentences) using techniques like NLTK, spaCy, or regular expressions is essential for further analysis.\n",
    "\n",
    "### 4. Removing Stop Words\n",
    "Removing common stop words using predefined lists from libraries like NLTK or spaCy can improve the efficiency of NLP models by focusing on meaningful words.\n",
    "\n",
    "### 5. Part-of-Speech Tagging\n",
    "Assigning grammatical categories like noun, verb, adjective to tokens using POS tagging helps in understanding the syntactic structure of text for advanceda analysis.\n",
    "nalysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716ffb5-9eac-437d-b218-586f7ff84e29",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5432d9b4-93ba-42cb-84f5-41f8ace6a0f9",
   "metadata": {},
   "source": [
    "One of the key steps in NLP is data preprocessing, which involves cleaning and preparing text data for analysis. In this notebook, we will focus on text cleaning, specifically removing HTML tags, URLs, special characters, non-alphabetical characters, and emojis from text data.\n",
    "\n",
    "Removing HTML Tags:\n",
    "HTML tags are commonly found in text data scraped from websites. These tags can interfere with NLP tasks by introducing noise into the data.\n",
    "Removing URLs:\n",
    "URLs are another common source of noise in text data.\n",
    "\n",
    "Removing Special Characters:\n",
    "Special characters such as punctuation marks can also introduce noise into text data.\n",
    "\n",
    "Removing Non-Alphabetical Characters:\n",
    "Non-alphabetical characters such as numbers can also be removed from text data to improve its quality for NLP tasks.\n",
    "\n",
    "Removing Emojis:\n",
    "Emojis are graphical symbols used to express emotions or ideas in digital communication. While emojis can add meaning to text data, they can also introduce noise into NLP tasks.\n",
    "\n",
    "To perform text cleaning for NLP tasks, including removing HTML tags, URLs, special characters, non-alphabetical characters, and emojis, we can create a Python function that applies a series of regex-based text transformations. Here's a function that combines these text cleaning steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b382906-b0d1-4caa-9c4f-46e0a17e3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and non-alphabetical characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.su\n",
    "    \n",
    "#Sample text with HTML, URLs, special characters, non-alphabetical characters, and emojis\n",
    "text = \"<p>Hello, World! This is an Example Text with special characters & emojis ðŸ˜Š https://example.com</p>\"\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "\n",
    "print(\"\\nText after cleaning:\")\n",
    "print(cleaned_text)(r'', text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4895cb-0e10-4ee8-8a27-d287a79213c9",
   "metadata": {},
   "source": [
    "In this function clean_text(), we use regular expressions to remove HTML tags, URLs, special characters, non-alphabetical characters, and emojis from the input text. The re.sub() function is used to substitute the matched patterns with empty strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775181f0-c247-43c7-bb42-8769d64a6678",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a39edb-25d8-4ccf-94ce-70072a1dc538",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming is a text processing technique in Natural Language Processing (NLP) that reduces words to their root or base form, known as the stem. This helps in simplifying the vocabulary and improving the efficiency of text analysis tasks such as text classification, information retrieval, and sentiment analysis.\n",
    "\n",
    "In NLP, stemming is often used to normalize words by removing suffixes and prefixes to convert them into their base form. For example, the words \"running\", \"runs\", and \"ran\" would all be stemmed to \"run\".\n",
    "\n",
    "There are different algorithms for stemming in NLP, with one of the most commonly used being the Porter Stemmer algorithm. The NLTK library in Python provides an implementation of the Porter Stemmer algorithm that can be used for stemming text data.\n",
    "\n",
    "Let's see an example of how to perform stemming using the NLTK library in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716cd46-045d-4e85-bbef-476a459c0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Sample text data\n",
    "text = \"Stemming is a text processing technique that reduces words to their base form.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Perform stemming on each word\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Original words:\")\n",
    "print(words)\n",
    "\n",
    "print(\"\\nStemmed words:\")\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b408002-945a-422a-b730-ada7e67835ab",
   "metadata": {},
   "source": [
    "In this code snippet, we first import the necessary modules from the NLTK library. We then initialize the Porter Stemmer and tokenize the sample text into individual words. Next, we apply stemming using the stem() method of the stemmer object on each word in the text. Finally, we print out the original words and their stemmed versions.\n",
    "\n",
    "By using stemming in NLP, we can reduce the vocabulary size and improve the performance of text analysis tasks by treating different forms of a word as the same entity. This can lead to more accurate and efficient text processing results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5d871-eb4e-4376-ae0e-fcede6bc856c",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization is another text processing technique in Natural Language Processing (NLP) that, like stemming, aims to reduce words to their base or root form. However, unlike stemming, lemmatization considers the context of the word and its part of speech to determine the correct lemma. This results in more accurate and meaningful base forms of words compared to stemming.\n",
    "\n",
    "In NLP, lemmatization is commonly used to normalize words by converting them to their dictionary form or lemma. For example, the words \"am\", \"are\", and \"is\" would all be lemmatized to \"be\".\n",
    "\n",
    "The NLTK library in Python provides an implementation of the WordNet Lemmatizer algorithm, which can be used for lemmatizing text data. Let's see an example of how to perform lemmatization using the NLTK library in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb5d26-8b0e-43fa-900a-71ec20352a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Sample text data\n",
    "text = \"Lemmatization is a text processing technique that reduces words to their base form.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Perform lemmatization on each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(\"Original words:\")\n",
    "print(words)\n",
    "\n",
    "print(\"\\nLemmatized words:\")\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd573f-e921-4a6b-9e99-dca7d2d7d5f5",
   "metadata": {},
   "source": [
    "In this code snippet, we first import the necessary modules from the NLTK library. We then initialize the WordNet Lemmatizer and tokenize the sample text into individual words. Next, we apply lemmatization using the lemmatize() method of the lemmatizer object on each word in the text. Finally, we print out the original words and their lemmatized versions.\n",
    "\n",
    "By using lemmatization in NLP, we can obtain more meaningful base forms of words that are linguistically accurate. This can help improve the performance of text analysis tasks by ensuring that words are normalized to their correct dictionary forms based on their context and part of speech.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7520dbc8-c36b-414b-a01d-725676a6968c",
   "metadata": {},
   "source": [
    "### Handling contractions\n",
    "Handling contractions is an important preprocessing step in Natural Language Processing (NLP) tasks, as contractions are commonly used in informal text but can complicate text analysis due to their non-standard forms. Here, we will discuss what contractions are, why they need to be handled, and how to handle them using Python.\n",
    "\n",
    "#### What are Contractions?\n",
    "\n",
    "Contractions are shortened versions of words or phrases that are formed by combining two words and replacing one or more letters with an apostrophe. For example, \"can't\" is a contraction of \"cannot\", and \"I'm\" is a contraction of \"I am\". Contractions are often used in informal writing, such as social media posts, text messages, and online forums.\n",
    "\n",
    "#### Why Handle Contractions in NLP?\n",
    "\n",
    "In NLP tasks like text classification, sentiment analysis, and machine translation, it is important to preprocess text data to ensure consistency and accuracy in analysis. Handling contractions is crucial because they can affect the performance of NLP models by introducing noise and ambiguity in the text. For example, the words \"can't\" and \"cannot\" should be treated as the same word for most NLP tasks.\n",
    "\n",
    "#### How to Handle Contractions in Python\n",
    "\n",
    "One common approach to handling contractions in NLP is to expand them into their full forms before further text processing. We can create a dictionary mapping contractions to their expanded forms and then use this dictionary to replace contractions in the text data. Let's see how this can be done using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719075e9-e664-44ef-9838-d08b458ac607",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "    \"ain't\": \"are not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"didn't\" : \"did not\",\n",
    "    \"It's\" : \"it is\"\n",
    "    # Add more contractions and their expansions as needed\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    for contraction, expansion in contractions_dict.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text\n",
    "\n",
    "# Sample text data with contractions\n",
    "text = \"I can't believe he didn't show up for the party. It's such a shame!\"\n",
    "\n",
    "# Expand contractions in the text\n",
    "expanded_text = expand_contractions(text, contractions_dict)\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "\n",
    "print(\"\\nText with expanded contractions:\")\n",
    "print(expanded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe1903-80d6-4b17-884b-82a1ceb23e31",
   "metadata": {},
   "source": [
    "In this code snippet, we define a dictionary contractions_dict that maps contractions to their expanded forms. We then define a function expand_contractions() that iterates over the dictionary and replaces each contraction with its expansion in the input text. Finally, we apply this function to a sample text with contractions and print out the original text and the text with expanded contractions.\n",
    "\n",
    "By handling contractions in this way, we can preprocess text data effectively before performing further NLP tasks such as tokenization, lemmatization, and sentiment analysis. This helps improve the accuracy and reliability of NLP models by ensuring that contractions are correctly interpreted in the text analysis process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705ee02-08c0-4276-8214-7b4895a22ecf",
   "metadata": {},
   "source": [
    "### Expanding abbreviations\n",
    "Expanding abbreviations is another important preprocessing step in Natural Language Processing (NLP) tasks, as abbreviations are commonly used in text but can introduce ambiguity and misunderstanding in text analysis. We will discuss what abbreviations are, why they need to be expanded, and how to expand them using Python.\n",
    "\n",
    "#### What are Abbreviations?\n",
    "\n",
    "Abbreviations are shortened forms of words or phrases that are commonly used to save time and space in writing. For example, \"etc.\" is an abbreviation of \"et cetera\", and \"Dr.\" is an abbreviation of \"Doctor\". Abbreviations are often used in formal and informal writing, including academic papers, business documents, and social media posts.\n",
    "\n",
    "#### Why Expand Abbreviations in NLP?\n",
    "\n",
    "In NLP tasks like text classification, entity recognition, and information retrieval, it is important to expand abbreviations to ensure that the text is correctly understood and analyzed. Abbreviations can have multiple meanings or interpretations, which can lead to errors in text processing and hinder the performance of NLP models. For example, \"C\" could stand for \"Celsius\", \"Carbon\", or \"Copyright\" depending on the context.\n",
    "\n",
    "#### How to Expand Abbreviations in Python\n",
    "\n",
    "One common approach to expanding abbreviations in NLP is to create a dictionary mapping abbreviations to their full forms and then use this dictionary to replace abbreviations in the text data. Let's see how this can be done using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b6817-5a4e-40b0-a40f-168997ddcae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations_dict = {\n",
    "    \"Dr.\": \"Doctor\",\n",
    "    \"etc.\": \"et cetera\",\n",
    "    \"Mr.\": \"Mister\"\n",
    "    # Add more abbreviations and their expansions as needed\n",
    "}\n",
    "\n",
    "def expand_abbreviations(text, abbreviations_dict):\n",
    "    for abbreviation, expansion in abbreviations_dict.items():\n",
    "        text = text.replace(abbreviation, expansion)\n",
    "    return text\n",
    "\n",
    "# Sample text data with abbreviations\n",
    "text = \"Dr. Smith will be joining us for the meeting, etc. Please confirm your attendance.\"\n",
    "\n",
    "# Expand abbreviations in the text\n",
    "expanded_text = expand_abbreviations(text, abbreviations_dict)\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "\n",
    "print(\"\\nText with expanded abbreviations:\")\n",
    "print(expanded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc356b8a-fe07-41e0-9dff-217803497ba8",
   "metadata": {},
   "source": [
    "In this code snippet, we define a dictionary abbreviations_dict that maps abbreviations to their full forms. We then define a function expand_abbreviations() that iterates over the dictionary and replaces each abbreviation with its expansion in the input text. Finally, we apply this function to a sample text with abbreviations and print out the original text and the text with expanded abbreviations.\n",
    "\n",
    "By expanding abbreviations in this way, we can preprocess text data effectively before performing further NLP tasks such as tokenization, named entity recognition, and document classification. This helps improve the accuracy and clarity of NLP models by ensuring that abbreviations are correctly interpreted in the text analysis process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636f016-9ce6-4d9e-928d-2462f8f2ed39",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "Lowercasing is a fundamental preprocessing step in Natural Language Processing (NLP) tasks that involves converting all text to lowercase. In this article, we will discuss the importance of lowercasing in NLP, its benefits, and how to implement lowercasing using Python.\n",
    "\n",
    "#### Why Lowercasing in NLP?\n",
    "\n",
    "Lowercasing is important in NLP for several reasons:\n",
    "\n",
    "1. **Normalization**: Lowercasing helps normalize the text data by ensuring that words are treated consistently. For example, \"Hello\" and \"hello\" should be considered the same word in text analysis.\n",
    "\n",
    "2. **Reducing Vocabulary Size**: Lowercasing reduces the vocabulary size by merging words with different capitalizations. This simplifies text processing and can improve the performance of NLP models.\n",
    "\n",
    "3. **Improved Generalization**: Lowercasing can help NLP models generalize better by focusing on the meaning of words rather than their capitalization.\n",
    "\n",
    "4. **Consistency in Text Analysis**: Lowercasing ensures consistency in text analysis tasks such as tokenization, word frequency analysis, and sentiment analysis.\n",
    "\n",
    "#### How to Implement Lowercasing in Python\n",
    "\n",
    "Lowercasing text data in Python is straightforward and can be done using the lower() method of strings. Let's see how to implement lowercasing in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1ba00-4456-4596-94fe-41ec9a58d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Sample text data with mixed case\n",
    "text = \"Hello, World! This is an Example Text with Mixed Case.\"\n",
    "\n",
    "# Lowercase the text\n",
    "lowercased_text = lowercase_text(text)\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "\n",
    "print(\"\\nText after lowercasing:\")\n",
    "print(lowercased_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f50b92e-0719-4c42-8a4e-b227ff50378e",
   "metadata": {},
   "source": [
    "In this code snippet, we define a function lowercase_text() that takes a text input and converts it to lowercase using the lower() method. We then apply this function to a sample text with mixed case and print out the original text and the text after lowercasing.\n",
    "\n",
    "By lowercasing text data before further processing in NLP tasks, we ensure that the text is normalized and consistent, which can lead to better performance of NLP models. Lowercasing is a simple yet effective preprocessing step that is commonly used in various NLP applications to improve text analysis and understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ddbd42-ca9d-43d7-a863-beec8b55dc8b",
   "metadata": {},
   "source": [
    "### Handling Numerical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ade04d-715c-4b8b-a81a-0d835e8f7811",
   "metadata": {},
   "source": [
    "When dealing with Natural Language Processing (NLP) tasks, textual data often contains a mix of both text and numerical values. Handling numerical values in NLP data preprocessing is essential to ensure that the data is appropriately formatted for machine learning models that expect only text inputs.\n",
    "\n",
    "#### Strategies for Handling Numerical Values\n",
    "\n",
    "#### 1. **Tokenization of Numerical Values**\n",
    "One common approach is to tokenize numerical values into separate tokens. This way, the numerical information is preserved, and the model can learn from it effectively. For instance, \"100 dollars\" could be tokenized as [\"100\", \"dollars\"].\n",
    "\n",
    "#### 2. **Normalization of Numerical Values**\n",
    "Normalization involves scaling numerical values to make them consistent across the dataset. Common techniques include min-max scaling or standardization. This helps in preventing numerical values from dominating the text features during model training.\n",
    "\n",
    "#### 3. **Replacing Numerical Values**\n",
    "In some cases, it might be beneficial to replace numerical values with a generic placeholder like \"<NUM>\" to treat them as a regular token. This can help prevent overfitting on specific numerical values and enable the model to learn more general patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75499f-7e3d-47c5-a31f-f7e8f061ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Python Code Examples for Handling Numerical Values\n",
    "\n",
    "### Tokenization of Numerical Values\n",
    "import re\n",
    "\n",
    "text = \"The product costs $50.50 and weighs 2.5 kg.\"\n",
    "tokenized_text = re.sub(r \\b\\d+(?:\\.\\d+)?\\b ,  <NUM> , text)\n",
    "print(tokenized_text)\n",
    "\n",
    "\n",
    "### Normalization of Numerical Values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [[10], [20], [30], [40], [50]]\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n",
    "\n",
    "\n",
    "### Replacing Numerical Values\n",
    "import re\n",
    "\n",
    "text = \"The document contains 100 pages and weighs 3.5 kg.\"\n",
    "normalized_text = re.sub(r \\b\\d+(?:\\.\\d+)?\\b ,  <NUM> , text)\n",
    "print(normalized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2c4b3a-fcf5-414c-ba2b-0ecf686bdb2c",
   "metadata": {},
   "source": [
    "## Text Tokenization\n",
    "In Natural Language Processing (NLP), tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, phrases, or symbols, depending on the granularity of the tokenization. Tokenization is a crucial step in NLP data preprocessing as it helps in standardizing text data and preparing it for further analysis or modeling. In this article, we will explore tokenization techniques and implement them using Python.\n",
    "\n",
    "### Tokenization Techniqus\n",
    "\n",
    "#### 1. Word Tokenization\n",
    "Word tokenization involves splitting text into individual words. It is the most common form of tokenization and serves as the basis for many NLP tasks.\n",
    "\n",
    "#### 2. Sentence Tokenization\n",
    "Sentence tokenization breaks text into individual sentences. This is useful for tasks that require analyzing text at the sentence level.\n",
    "\n",
    "#### 3. Regular Expression Tokenization\n",
    "Regular expression tokenization allows for more customized tokenization based on specific patterns or rules defined using regular expressions.\n",
    "\n",
    "#### Implementing Tokenizatin in Python\n",
    "\n",
    "Let s use the popular nltk library in Python to demonstrate how to perform tokenization on ans generated by each tokenization technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83148ea8-e764-40ea-bfcc-77561fa235bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Example\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sample text for tokenization\n",
    "text = \"Tokenization is an important step in NLP. It helps in breaking down text into smaller units.\"\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"Word Tokens:\")\n",
    "print(words)\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"\\nSentence Tokens:\")\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8502c8-7437-4dea-a6c1-7bf5b95dffbe",
   "metadata": {},
   "source": [
    "In this code snippet:\n",
    "- We import the necessary functions from the nltk.tokenize module.\n",
    "- We define a sample text for tokenization.\n",
    "- We perform word tokenization using word_tokenize() and sentence tokenization using sent_tokenize().\n",
    "- Finally, we print out the tokens generated by each tokenization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a9f013-00d3-4101-8f06-8d60a68fc6e6",
   "metadata": {},
   "source": [
    "## Removing Stop Words\n",
    "\n",
    "When working with Natural Language Processing (NLP) tasks, one common preprocessing step is the removal of stop words. Stop words are words that are considered to be too common and do not add much value to the meaning of a sentence. Examples of stop words include  the ,  is ,  and ,  in ,  at  etc. Removing stop words can help in reducing the dimensionality of the dataset and improving the efficiency of the downstream NLP tasks like text classification, sentiment analysis, and named entity recognition.\n",
    "\n",
    "#### Python Code for Removing Stop Words\n",
    "To remove stop words from a piece of text in Python, we can use libraries like NLTK (Natural Language Toolkit) or spaCy that provide predefined lists of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6f339-7444-4444-b3d8-25ea0aa3bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using NLTK:\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download( stopwords )\n",
    "nltk.download( punkt )\n",
    "\n",
    "text = \"This is an example sentence showing stop words removal.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "stop_words = set(stopwords.words( english ))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "filtered_text =    .join(filtered_tokens)\n",
    "print(filtered_text)\n",
    "\n",
    "\n",
    "### Using spaCy:\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"This is an example sentence showing stop words removal.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "filtered_text =    .join(filtered_tokens)\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de207541-da0d-4cbd-9483-98df29da3117",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging\n",
    "Part-of-Speech (POS) tagging is a fundamental task in Natural Language Processing (NLP) that involves identifying the grammatical information of each word in a text, such as whether it is a noun, verb, adjective, adverb, etc. Proper preprocessing techniques for POS tagging are crucial for various NLP tasks like information extraction, sentiment analysis, and machine translation. In this article, we will delve into the importance of POS tagging and explore Python code examples for POS tagging data preprocessing.\n",
    "\n",
    "### Importance of Part-of-Speech Tagging\n",
    "\n",
    "### 1. **Semantic Analysis**\n",
    "POS tagging helps in understanding the meaning and context of words within a sentence. By knowing the part of speech of each word, we can infer relationships between words and extract valuable semantic information.\n",
    "\n",
    "### 2. **Syntactic Parsing**\n",
    "POS tagging aids in syntactic parsing by providing grammar cues and identifying the role of each word in a sentence. This information is essential for constructing parse trees and extracting the syntactic structure of a sentence.\n",
    "\n",
    "### 3. **Information Extraction**\n",
    "POS tagging plays a crucial role in information extraction tasks by identifying entities, attributes, and relationships based on the grammatical categories of words. It helps in extracting structured information from unstructured tex#t data.\n",
    "\n",
    "## Python Code Examples for Part-of-Speech (POS) Tagging\n",
    "\n",
    "Let s use the popular Natural Language Processing library NLTK to demonstrate POS taggien in doc:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6beec-207c-46f4-bb14-91f6b0e83803",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization and Part-of-Speech Tagging with NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Part-of-speech tagging is important for NLP tasks.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(pos_tags)\n",
    "\n",
    "\n",
    "### Using spaCy for POS Tagging\n",
    "Let s explore POS tagging using spaCy, a powerful NLP library:\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"I like to eat pizza with extra cheese.\"\n",
    "\n",
    "# POS Tagging with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2efdc9-4267-4adc-83e0-1c7006a19552",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c9622-c9d9-4af3-b47d-7d706a73ed3a",
   "metadata": {},
   "source": [
    "Feature extraction is a crucial step in natural language processing (NLP) that involves transforming raw text data into a format that can be easily understood and analyzed by machine learning algorithms. By extracting relevant features from text data, NLP models can better understand the underlying patterns and relationships within the text, leading to more accurate and meaningful results. \n",
    "Two common approaches to feature extraction in NLP are sparse vector representations and dense vector representations. In this notebook, we will explore these techniques, their methods, and their applications in NLP.\n",
    "HerE are some of common feature extraction methods in NLP:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbabb952-1a45-46a0-9fa6-e9c6ebf69837",
   "metadata": {},
   "source": [
    "### Sparse Vector Representations\n",
    "\n",
    "Sparse vector representations are binary vectors where each dimension corresponds to a unique word in the vocabulary. The vector is sparse because most dimensions are zero, except for the dimensions corresponding to the words present in the text. One-hot encoding is a classic example of sparse vector representation, where each word is represented by a vector with a single non-zero element.\n",
    "\n",
    "#### Method:\n",
    "1. Tokenization: Split the text into individual words or tokens.\n",
    "2. Vocabulary Creation: Build a vocabulary by assigning a unique index to each word.\n",
    "3. One-Hot Encoding: Represent each word as a binary vector with all zeros except for the index corresponding to the word.\n",
    "\n",
    "#### Applications:\n",
    "- Bag-of-Words (BoW): Represent documents as sparse vectors of word frequencies.\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF): Weight words based on their importance in a document collection.\n",
    "\n",
    "### Dense Vector Representations\n",
    "\n",
    "Dense vector representations, also known as word embeddings, encode semantic information about words in continuous vector spaces. Unlike sparse vectors, dense vectors capture relationships between words based on their context in a text corpus. Word2Vec, GloVe, and FastText are popular techniques for learning dense word embeddings.\n",
    "\n",
    "#### Method:\n",
    "1. Training Word Embeddings: Learn word representations by considering the context in which words appear in a large corpus of text data.\n",
    "2. Vector Space Representation: Encode semantic relationships between words in a continuous vector space with lower dimensions compared to the vocabulary size.\n",
    "\n",
    "#### Applications:\n",
    "- Semantic Similarity: Measure the similarity between words based on their vector representations.\n",
    "- Text Classification: Use pre-trained word embeddings to improve performance on classification tasks.\n",
    "- Named Entity Recognition: Leverage word embeddings to capture contextual information for entity recognition.\n",
    "\n",
    "#### Choosing Between Sparse and Dense Representations\n",
    "\n",
    "- Sparse Vectors are simple and easy to interpret but may struggle with capturing semantic relationships between words.\n",
    "- Dense Vectors provide richer representations that enable models to learn from context and generalize better across tasks.\n",
    "\n",
    "When deciding between sparse and dense representations, consider the complexity of your NLP task, the availability of training data, and the computational resources required for training and inference.\n",
    "Sparse and dense vector representations are fundamental techniques in NLP for converting textual data into numerical features. While sparse vectors are straightforward and interpretable, dense vectors offer richer semantic information that enhances the performance of NLP models.\n",
    "\n",
    "Experiment with both sparse and dense feature extraction techniques in your NLP projects to determine which approach best suits your specific tasks and datasets. Stay curious, explore different methods, and leverage the power of feature extraction to unlock the potential of NLP applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3da3cb-e47d-4340-8593-3902e645b654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca2862a9-f749-42b9-b0e9-6eb63ae97950",
   "metadata": {},
   "source": [
    "## Feature Extraction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf24a8-3fc6-4084-b373-91ef505b42c2",
   "metadata": {},
   "source": [
    "There are several techniques for feature extraction in NLP, each with its own advantages and limitatio and the choice of feature extraction technique "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868ef66-4b90-4bd9-8fd8-0cec6517a944",
   "metadata": {},
   "source": [
    "## Bag of Words (Bow):\n",
    "\n",
    "In Natural Language Processing (NLP), the Bag of Words (BoW) model is a simple and commonly used technique for text representation. It represents text data as a collection of unique words and their frequencies in a document, disregarding grammar and word order. This model is widely used for tasks like text classification, sentiment analysis, and information retrieval.\n",
    "\n",
    "#### How Bag of Words Works\n",
    "\n",
    "The Bag of Words model involves the following steps:\n",
    "\n",
    "1. **Tokenization**: The text is split into individual words or tokens.\n",
    "2. **Vocabulary Creation**: A vocabulary is created by listing all unique words in the corpus.\n",
    "3. **Vectorization**: Each document is represented as a vector, where each element corresponds to the frequency of a word in the vocabulary.\n",
    "\n",
    "Let s see how to implement the Bag of Words model in Python using the CountVectorizer class from the sklearn.feature_extraction.text module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9a375a-8941-4f41-9991-2a94fdb4488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "     This is the first document. ,\n",
    "     This document is the second document. ,\n",
    "     And this is the third one. ,\n",
    "     Is this the first document? ,\n",
    "]\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense array for better visualization\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f579a9-664e-42d8-8c91-e2ab6de3c048",
   "metadata": {},
   "source": [
    "In this code snippet, we first define a sample corpus of text documents. We then create an instance of CountVectorizer and fit it on the corpus to learn the vocabulary and transform the documents into a matrix of word frequencies.\n",
    "\n",
    "### Benefits and Limitations of Bag of Words\n",
    "\n",
    "#### Benefits:\n",
    "- Simple and easy to implement.\n",
    "- Captures the frequency information of words.\n",
    "- Suitable for large datasets and high-dimensional feature spaces.\n",
    "\n",
    "#### Limitations:\n",
    "- Ignores word order and context.\n",
    "- Treats all words as independent features.\n",
    "- Increases the dimensionality of the feature space.\n",
    "\n",
    "The Bag of Words model is a fundamental technique in NLP for representing text data as numerical vectors. While it has its limitations, it serves as a useful starting point for many text-based tasks.\n",
    "\n",
    "Feel free to explore further by experimenting with different parameters of CountVectorizer and incorporating additional preprocessing steps like stop-word removal and stemming to enhance the BoW model s performance..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86aef0-89a9-451d-b596-69409dcd61bb",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a widely used technique for text representation that helps in understanding the importance of words in a document relative to a collection of documents. TF-IDF combines two metrics: term frequency (TF) and inverse document frequency (IDF) to assign weights to words in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee79859-4480-42c1-8e53-5ad1a015a331",
   "metadata": {},
   "source": [
    "### How TF-IDF Works\n",
    "\n",
    "The TF-IDF formula for a term in a document is calculated as follows:\n",
    "\n",
    "TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)\n",
    "\n",
    "- **Term Frequency (TF)**: Measures how often a term appears in a document. It is calculated as the number of times a term appears in a document divided by the total number of terms in the document.\n",
    "\n",
    "- **Inverse Document Frequency (IDF)**: Measures the rarity of a term across all documents. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the term.\n",
    "  \n",
    "Let s see how to implement TF-IDF in Python using the TfidfVectorizer class from the sklearn.feature_extraction.text module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5774603-9a5e-40b7-9f6d-d969470450a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "     This is the first document. ,\n",
    "     This document is the second document. ,\n",
    "     And this is the third one. ,\n",
    "     Is this the first document? ,\n",
    "]\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense array for better visualization\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762c1c3-3c1e-4be0-acda-1c19d0c392a6",
   "metadata": {},
   "source": [
    "In this code snippet, we use TfidfVectorizer to convert the text documents into TF-IDF weighted vectors. The resulting matrix represents each document as a vector of TF-IDF values for each word in the vocabulary.\n",
    "\n",
    "### Benefits and Limitations of TF-IDF\n",
    "\n",
    "#### Benefits:\n",
    "- Considers both the frequency and rarity of terms.\n",
    "- Helps in identifying important words in a document.\n",
    "- Suitable for tasks like information retrieval and text classification.\n",
    "\n",
    "#### Limitations:\n",
    "- Ignores word order and context.\n",
    "- Treats all words as independent features.\n",
    "- May not capture semantic relationships between words.\n",
    "\n",
    "TF-IDF is a powerful technique in NLP for capturing the importance of words in documents. It provides a way to represent text data with weighted features that reflect their significance.\n",
    "\n",
    "To enhance the performance of TF-IDF, consider tuning parameters like n-grams, stop-word removal, and stemming. Experiment with different settings to optimize the TF-IDF representation for your specific NLP task.\n",
    "\n",
    "Feel free to explore further by applying TF-IDF to larger datasets and experimenting with different text preprocessing techniques to improve the quality of the feature representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5027adf-0b0f-4714-a8a4-3343f2f1ac00",
   "metadata": {},
   "source": [
    "## Word Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7e7ee-c67e-416f-8eea-0a199dfa04ac",
   "metadata": {},
   "source": [
    "Word embeddings are a key technique for representing words as dense vectors in a high-dimensional space. Word embeddings capture semantic relationships between words and enable NLP models to learn from the contextual meaning of words. In this notebook, we will explore word embeddings and demonstrate how to use them in Python with the popular gensim library.\n",
    "\n",
    "### What are Word Embeddings?\n",
    "\n",
    "Word embeddings are dense vector representations of words in a continuous vector space. Unlike traditional one-hot encoding, where each word is represented as a sparse binary vector, word embeddings encode semantic information about words based on their context in a text corpus. This allows NLP models to better understand the meaning and relationships between words.\n",
    "\n",
    "Popular word embedding techniques include Word2Vec, GloVe, and FastText, which learn word representations by considering the context in which words appear in a large corpus of text data.\n",
    "\n",
    "### Using Word2Vec with Gensim\n",
    "\n",
    "Let s see how to train and use Word2Vec word embeddings in Python using the gensim library:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cbe0e1-8423-4938-b6f5-6cf19bb0fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "     This is the first sentence. ,\n",
    "     This is the second sentence. ,\n",
    "     And this is the third sentence. ,\n",
    "]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Get the vector representation of a word\n",
    "word_vector = model.wv[ sentence ]\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar( first )\n",
    "\n",
    "print(\"Vector representation of  sentence :\", word_vector)\n",
    "print(\"Similar words to  first :\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877eb8cd-1c4a-4512-bc9b-a80302349a4f",
   "metadata": {},
   "source": [
    "In this code snippet, we tokenize the corpus, train a Word2Vec model using gensim, and demonstrate how to get the vector representation of a word and find similar words based on the learned embeddings.\n",
    "\n",
    "### Benefits of Word Embeddings\n",
    "\n",
    "- Capture semantic relationships between words.\n",
    "- Enable NLP models to generalize better across different tasks.\n",
    "- Reduce dimensionality compared to sparse one-hot encodings.\n",
    "- Improve performance in tasks like sentiment analysis, named entity recognition, and machine translation.\n",
    "\n",
    "Word embeddings play a crucial role in NLP by providing dense representations of words that encode semantic information. By leveraging word embeddings, NLP models can better understand the meaning of words and improve their performance on various text-related tasks.\n",
    "\n",
    "Experiment with different word embedding techniques and hyperparameters to optimize the performance of your NLP models. Additionally, consider pre-trained word embeddings like Word2Vec, GloVe, or FastText for tasks with limited training data.\n",
    "Explore more advanced concepts like contextual word embeddings (e.g., BERT, ELMO) and transfer learning to enhance the capabilities of your NLP models further. Stay curious and keep exploring the fascinating world of word embeddings in NLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8890cb09-60d6-4f43-96c2-f2a576f015b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
